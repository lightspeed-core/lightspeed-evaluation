# GEval Metric Registry
# Define reusable GEval metrics here to avoid repetition in evaluation scenarios.
# These metrics can be referenced in evaluation YAMLs using "geval:<metric_name>"
# without needing to repeat the full configuration.
# Caveat: These are generated metrics and are not recommended for production without
# prior verification.

# ==============================================================================
# TURN-LEVEL METRICS
# ==============================================================================

technical_accuracy:
  criteria: |
    Assess whether the response provides technically accurate Ansible commands,
    playbook syntax, and follows Red Hat best practices. The response should
    contain valid YAML syntax and appropriate Ansible modules.
  evaluation_params:
    - input
    - actual_output
    - expected_output
  evaluation_steps:
    - "Verify that the Ansible syntax is valid and follows YAML formatting rules"
    - "Check if the response uses appropriate Ansible modules and parameters"
    - "Assess whether the solution aligns with Red Hat Ansible documentation"
    - "Verify the response addresses the user's specific query or task"
    - "Check for potential security issues or anti-patterns"
  threshold: 0.7

command_validity:
  criteria: |
    Evaluate whether the generated commands or playbook tasks are syntactically
    correct and would execute successfully in a real Ansible environment.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Verify proper YAML indentation and structure"
    - "Check that module names are valid and correctly spelled"
    - "Ensure required parameters for modules are present"
    - "Validate that variables and facts are properly referenced"
  threshold: 0.8

ansible_best_practices:
  criteria: |
    Determine whether the response follows Ansible best practices including
    idempotency, task naming, handler usage, and role organization.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Check if tasks are idempotent (can be run multiple times safely)"
    - "Verify tasks have descriptive names"
    - "Assess proper use of handlers for service restarts"
    - "Check for proper variable naming conventions"
    - "Verify appropriate use of when conditions and loops"
  threshold: 0.7

security_awareness:
  criteria: |
    Evaluate whether the response demonstrates security awareness and avoids
    common security pitfalls in Ansible automation.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Check for hardcoded credentials or sensitive data"
    - "Verify proper use of Ansible Vault references where needed"
    - "Assess appropriate file permissions in file/template modules"
    - "Check for secure defaults in configurations"
    - "Verify no_log usage for sensitive tasks"
  threshold: 0.8

# ==============================================================================
# CONVERSATION-LEVEL METRICS
# ==============================================================================

conversation_coherence:
  criteria: |
    Evaluate whether the conversation maintains context and provides coherent
    responses across multiple turns. The assistant should reference previous
    exchanges and build upon earlier context.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Check if the assistant remembers information from previous turns"
    - "Verify responses build logically on previous context"
    - "Assess whether the conversation flows naturally"
    - "Check for contradictions with earlier statements"
  threshold: 0.6

task_completion:
  criteria: |
    Assess whether the conversation successfully helps the user complete their
    intended Ansible automation task from start to finish.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Determine if the user's original goal was identified"
    - "Check if all necessary steps were provided"
    - "Verify the solution is complete and actionable"
    - "Assess if follow-up questions were addressed"
  threshold: 0.7

progressive_refinement:
  criteria: |
    Evaluate whether the conversation demonstrates progressive improvement and
    refinement of the Ansible solution based on user feedback and clarifications.
  evaluation_params:
    - input
    - actual_output
  evaluation_steps:
    - "Check if responses incorporate user feedback"
    - "Verify solutions become more specific over turns"
    - "Assess whether earlier mistakes are corrected"
    - "Check if the assistant adapts to user skill level"
  threshold: 0.6

# ==============================================================================
# EXAMPLE USAGE
# ==============================================================================
#
# In your evaluation_data.yaml, reference these metrics as:
#
# turn_metrics:
#   - "geval:technical_accuracy"
#   - "geval:command_validity"
#   - "geval:ansible_best_practices"
#
# conversation_metrics:
#   - "geval:conversation_coherence"
#   - "geval:task_completion"
#
# You can also override these definitions at runtime using turn_metrics_metadata
# or conversation_metrics_metadata in your evaluation data.
