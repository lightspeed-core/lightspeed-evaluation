# LightSpeed Evaluation Framework Configuration

# LLM as a judge configuration
llm:
  provider: "openai"          # LLM Provider (openai, watsonx, gemini, hosted_vllm etc..)
  model: "gpt-4o-mini"        # Model name for the provider
  temperature: 0.0            # Generation temperature
  max_tokens: 512             # Maximum tokens in response
  timeout: 300                # Request timeout in seconds
  num_retries: 3              # Retry attempts
  cache_dir: ".caches/llm_cache"      # Directory with LLM cache
  cache_enabled: true         # Is LLM cache enabled?

# Default embedding (for LLM as a judge) configuration:
embedding:
  provider: "openai"
  model: "text-embedding-3-small"
  provider_kwargs: {}
  cache_dir: ".caches/embedding_cache"
  cache_enabled: true


# API Configuration
# To get real time data. Currently it supports lightspeed-stack API.
# But can be easily integrated with other APIs with minimal change.
api:
  enabled: true                        # Enable API calls instead of using pre-filled data
  api_base: http://localhost:8080      # Base API URL
  endpoint_type: streaming             # Use "streaming" or "query" endpoint
  timeout: 300                         # API request timeout in seconds

  # API input configuration
  provider: "openai"                   # LLM provider for queries
  model: "gpt-4o-mini"                 # Model to use for queries
  no_tools: null                       # Whether to bypass tools and MCP servers (optional)
  system_prompt: null                  # System prompt (default None)

  cache_dir: ".caches/api_cache"  # Directory with lightspeed-stack cache
  cache_enabled: true                  # Is lightspeed-stack cache enabled?
  # Authentication via API_KEY environment variable only for MCP server

# Default metrics metadata
metrics_metadata:
  # Turn-level metrics metadata
  turn_level:
    # Ragas Response Evaluation metrics
    "ragas:response_relevancy":
      threshold: 0.8
      description: "How relevant the response is to the question"
      default: true  # This metric is applied by default when no turn_metrics specified

    "ragas:faithfulness":
      threshold: 0.8
      description: "How faithful the response is to the provided context"
      default: false  # By default the value is false

    # Ragas Context/Retrieval Evaluation metrics
    "ragas:context_recall":
      threshold: 0.8
      description: "Did we fetch every fact the answer needs?"

    "ragas:context_precision_with_reference":
      threshold: 0.7
      description: "How precise the retrieved context is (with reference)"

    "ragas:context_precision_without_reference":
      threshold: 0.7
      description: "How precise the retrieved context is (without reference)"

    "ragas:context_relevance":
      threshold: 0.7
      description: "Is what we retrieved actually relevant to user query?"

    # Custom metrics
    "custom:answer_correctness":
      threshold: 0.75
      description: "Correctness vs expected answer using custom LLM evaluation"

    "custom:intent_eval":
      threshold: 1  # boolean eval (either 0 or 1)
      description: "Intent alignment evaluation using custom LLM evaluation"

    "custom:tool_eval":
      description: "Tool call evaluation comparing expected vs actual tool calls"

    "custom:multiple_choice_exact":
      threshold: 1.0
      description: "MMLU-style multiple choice exact match with flexible letter extraction"

    "custom:multiple_choice_strict":
      threshold: 1.0
      description: "MMLU-style multiple choice strict match (single letter only)"

    # Script-based metrics
    "script:action_eval":
      description: "Script-based evaluation for infrastructure/environment validation"

  # Conversation-level metrics metadata
  conversation_level:
    # DeepEval metrics
    "deepeval:conversation_completeness":
      threshold: 0.8
      description: "How completely the conversation addresses user intentions"
      default: false

    "deepeval:conversation_relevancy":
      threshold: 0.7
      description: "How relevant the conversation is to the topic/context"

    "deepeval:knowledge_retention":
      threshold: 0.7
      description: "How well the model retains information from previous turns"

# Output Configuration
output:
  output_dir: "./eval_output"
  base_filename: "evaluation"
  enabled_outputs:          # Enable specific output types
    - csv                   # Detailed results CSV
    - json                  # Summary JSON with statistics
    - txt                   # Human-readable summary

  # CSV columns to include
  csv_columns:
    - "conversation_group_id"
    - "turn_id"
    - "metric_identifier"
    - "score"
    - "threshold"
    - "result"
    - "reason"
    - "query"
    - "response"
    - "execution_time"

# Visualization settings
visualization:
  figsize: [12, 8]            # Graph size (width, height)
  dpi: 300                    # Image resolution

  # Graph types to generate
  enabled_graphs:
    - "pass_rates"            # Pass rate bar chart
    - "score_distribution"    # Score distribution box plot
    - "conversation_heatmap"  # Heatmap of conversation performance
    - "status_breakdown"      # Pie chart for pass/fail/error breakdown

# Environment Variables - Automatically get set before any imports
environment:
  DEEPEVAL_TELEMETRY_OPT_OUT: "YES"        # Disable DeepEval telemetry
  DEEPEVAL_DISABLE_PROGRESS_BAR: "YES"     # Disable DeepEval progress bars

  LITELLM_LOG: ERROR                       # Suppress LiteLLM verbose logging

# Logging Configuration
logging:
  # Source code logging level
  source_level: INFO          # DEBUG, INFO, WARNING, ERROR, CRITICAL

  # Package logging level (imported libraries)
  package_level: ERROR

  # Log format and display options
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  show_timestamps: true

  # Specific package log levels (override package_level for specific libraries)
  package_overrides:
    httpx: ERROR
    urllib3: ERROR
    requests: ERROR
    matplotlib: ERROR
    LiteLLM: WARNING
    DeepEval: WARNING
    ragas: WARNING
