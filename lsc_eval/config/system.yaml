# LSC Evaluation Framework Configuration

# LLM Configuration
llm:
  provider: "openai"          # openai, anthropic, watsonx, azure, groq, gemini, ollama
  model: "gpt-4o-mini"        # Model name for the provider
  temperature: 0.0            # Generation temperature (0.0 = deterministic)
  max_tokens: 512             # Maximum tokens in response
  timeout: 300                # Request timeout in seconds
  num_retries: 3              # Retry attempts

# Lightspeed RAG Configuration TODO: Perhaps we don't need separate from above?
lightspeed_rag:
  base_url: "http://localhost:8080"    # Lightspeed Stack RAG API endpoint
  provider: "openai"                   # LLM provider for RAG queries
  model: "gpt-4o-mini"                 # Model to use for RAG queries
  system_prompt: "You are a helpful assistant with access to a knowledge base. Use the provided context to answer questions accurately and comprehensively."
  no_tools: false                      # Whether to bypass tools and MCP servers
  timeout: 300                         # Request timeout for RAG queries

# Environment Variables - Set before any imports
environment:
  DEEPEVAL_TELEMETRY_OPT_OUT: "YES"        # Disable DeepEval telemetry
  DEEPEVAL_DISABLE_PROGRESS_BAR: "YES"     # Disable DeepEval progress bars
  
  LITELLM_LOG_LEVEL: "ERROR"               # Suppress LiteLLM verbose logging

# Logging Configuration
logging:
  # Source code logging level
  source_level: "INFO"        # DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Package logging level (imported libraries)
  package_level: "ERROR"
  
  # Log format and display options
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  show_timestamps: true
  
  # Specific package log levels (override package_level for specific libraries)
  package_overrides:
    "httpx": "ERROR"
    "urllib3": "ERROR"
    "requests": "ERROR"
    "matplotlib": "ERROR"
    "openai": "ERROR"
    "LiteLLM": "WARNING"
    "DeepEval": "WARNING"

# Default metrics metadata
metrics_metadata:
  # Turn-level metrics metadata
  turn_level:
    # Ragas Response Evaluation metrics
    "ragas:faithfulness":
      threshold: 0.8
      type: "turn"
      description: "How faithful the response is to the provided context"
      framework: "ragas"
      
    "ragas:response_relevancy":
      threshold: 0.8
      type: "turn"
      description: "How relevant the response is to the question"
      framework: "ragas"
    
    # Ragas Context/Retrieval Evaluation metrics
    "ragas:context_recall":
      threshold: 0.8
      type: "turn"
      description: "Did we fetch every fact the answer needs?"
      framework: "ragas"
      
    "ragas:context_relevance":
      threshold: 0.7
      type: "turn"
      description: "Is what we retrieved actually relevant to user query?"
      framework: "ragas"
      
    "ragas:context_precision_with_reference":
      threshold: 0.7
      type: "turn"
      description: "How precise the retrieved context is (with reference)"
      framework: "ragas"
      
    "ragas:context_precision_without_reference":
      threshold: 0.7
      type: "turn"
      description: "How precise the retrieved context is (without reference)"
      framework: "ragas"
    
    # Custom metrics
    "custom:answer_correctness":
      threshold: 0.75
      type: "turn"
      description: "Correctness vs expected answer using custom LLM evaluation"
      framework: "custom"
      
    "custom:rag_response_evaluation":
      threshold: 0.7 #not sure about threshold here, kept similar to others
      type: "turn"
      description: "Evaluation of RAG response compared to original"
      framework: "custom"
  
  # Conversation-level metrics metadata
  conversation_level:
    # DeepEval metrics
    "deepeval:conversation_completeness":
      threshold: 0.8
      type: "conversation"
      description: "How completely the conversation addresses user intentions"
      framework: "deepeval"
      
    "deepeval:conversation_relevancy":
      threshold: 0.7
      type: "conversation" 
      description: "How relevant the conversation is to the topic/context"
      framework: "deepeval"
      
    "deepeval:knowledge_retention":
      threshold: 0.7
      type: "conversation"
      description: "How well the model retains information from previous turns"
      framework: "deepeval"

# Output Configuration
output:
  base_directory: "./eval_output"
  base_filename: "evaluation"
  formats:
    csv: true               # Detailed results CSV
    json: true              # Summary JSON
    txt: true               # Human-readable summary
  include_graphs: true      # Generate visualization graphs
  
  # CSV columns to include
  csv_columns:
    - "conversation_group_id"
    - "turn_id" 
    - "metric_identifier"
    - "score"
    - "threshold"
    - "result"
    - "reason"
    - "query"
    - "response"
    - "execution_time"

# Visualization settings
visualization:
  figsize: [12, 8]            # Graph size (width, height)
  dpi: 300                    # Image resolution
  
  # Graph types to generate
  enabled_graphs:
    - "pass_rates"            # Pass rate bar chart
    - "score_distribution"    # Score distribution box plot
    - "conversation_heatmap"  # Heatmap of conversation performance
    - "status_breakdown"      # Pie chart for pass/fail/error breakdown
