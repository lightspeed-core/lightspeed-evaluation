[project]
name = "lightspeed-evaluation"
version = "0.4.0"
description = "LSC Evaluation Framework - Comprehensive evaluation tooling for GenAI applications"
authors = []
requires-python = ">=3.11,<3.14"
readme = "README.md"
license = {text = "Apache"}

dependencies = [
    # Core evaluation framework dependencies
    "ragas>=0.3.0",
    "deepeval>=1.3.0",
    "litellm>=1.0.0",
    "pydantic>=2.0.0",
    "pyyaml>=6.0",
    "pandas>=2.1.4",
    "datasets>=2.0.0",
    "matplotlib>=3.5.0",
    "seaborn>=0.11.0",
    "numpy>=1.23.0",
    "scipy>=1.10.0",
    # Agent evaluation dependencies (for future integration)
    "httpx>=0.27.2",
    "tqdm>=4.67.1",
    # Generate answers dependencies
    "click>=8.0.0",
    "diskcache>=5.6.3",
    "tenacity>=9.1.2",
    "langchain[huggingface]>=0.3.27",
    "langchain-google-genai>=2.0.0",
]

[project.optional-dependencies]
# Local embedding models - includes torch (~6GB with CUDA, ~1.2GB CPU-only)
# Only needed when using embedding.provider: huggingface
# For CPU-only installation (smaller footprint), first install PyTorch CPU:
#   pip install torch --index-url https://download.pytorch.org/whl/cpu
#   pip install 'lightspeed-evaluation[local-embeddings]' --no-deps
#   pip install sentence-transformers --no-deps
# or
#   uv sync --extra local-embeddings
local-embeddings = [
    "sentence-transformers>=5.1.0",
]

# NLP metrics dependencies - required for nlp:bleu, nlp:rouge, nlp:semantic_similarity_distance
# Install with:
#   pip install 'lightspeed-evaluation[nlp-metrics]'
# or
#   uv sync --extra nlp-metrics
nlp-metrics = [
    "sacrebleu>=2.0.0",      # Required for BLEU score
    "rouge-score>=0.1.2",    # Required for ROUGE score
    "rapidfuzz>=3.0.0",      # Required for semantic_similarity_distance
]

[dependency-groups]
dev = [
    "bandit>=1.7.0",
    "black>=25.1.0",
    "mypy>=1.15.0",
    "ruff>=0.8.0",
    "pyright>=1.1.401",
    "pydocstyle>=6.3.0",
    "pylint-pydantic>=0.3.0",
    "pytest>=8.3.2",
    "pytest-cov>=5.0.0",
    "pytest-mock>=3.15.1",
]

[project.scripts]
lightspeed-eval = "lightspeed_evaluation.runner.evaluation:main"
generate_answers = "generate_answers.generate_answers:main"

# Note: torch[cpu] variant configuration removed for uv compatibility
# Modern PyTorch versions are available on PyPI directly

[tool.isort]
src_paths = ["src", "tests"]

[tool.black]
line-length = 88

[tool.pydocstyle]
convention = "google"

[tool.mypy]
disable_error_code = ["union-attr", "return-value", "arg-type", "import-untyped"]
ignore_missing_imports = true
plugins = ["pydantic.mypy"]

[tool.pydantic-mypy]
init_forbid_extra = true
init_typed = true
warn_required_dynamic_aliases = true

[tool.pylint.MASTER]
load-plugins = ["pylint_pydantic"]
init-hook = "import sys; sys.path.append('.')"

[tool.ruff]
[tool.ruff.lint.flake8-tidy-imports]
banned-api = { "unittest" = { msg = "use pytest instead of unittest" }, "unittest.mock" = { msg = "use pytest-mock instead of unittest.mock" } }

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
[tool.hatch.build.targets.wheel]
packages = ["src/lightspeed_evaluation"]
