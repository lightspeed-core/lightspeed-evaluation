"""LightSpeed Evaluation Framework - Main Evaluation Runner."""

import argparse
import shutil
import sys
import traceback
from pathlib import Path
from typing import Any, Optional

from lightspeed_evaluation.core.models.system import SystemConfig

# Import only lightweight modules at top level
from lightspeed_evaluation.core.system import ConfigLoader
from lightspeed_evaluation.core.system.exceptions import DataValidationError


def _clear_caches(system_config: SystemConfig) -> None:
    """Clear all cache directories for warmup mode.

    Args:
        system_config: System configuration containing cache directory paths
    """
    cache_dirs = []

    # Collect all enabled cache directories
    if system_config.llm.cache_enabled:
        cache_dirs.append(("LLM Judge", system_config.llm.cache_dir))
    # We clear the api cache even if the Lightspeed core api is disabled
    if system_config.api.cache_enabled:
        cache_dirs.append(("API", system_config.api.cache_dir))
    if system_config.embedding.cache_enabled:
        cache_dirs.append(("Embedding", system_config.embedding.cache_dir))

    if not cache_dirs:
        print("   No caches enabled to clear")
        return

    # Clear each cache directory
    for cache_name, cache_dir in cache_dirs:
        path = Path(cache_dir)
        if path.exists():
            shutil.rmtree(path)
            print(f"   Cleared {cache_name} cache: {cache_dir}")
        # Recreate empty directory
        path.mkdir(parents=True, exist_ok=True)


def _print_summary(
    summary: dict[str, Any],
    api_tokens: Optional[dict[str, int]] = None,
) -> None:
    """Print evaluation summary and token usage."""
    print(
        f"âœ… Pass: {summary['PASS']}, âŒ Fail: {summary['FAIL']}, "
        f"âš ï¸ Error: {summary['ERROR']}, â­ï¸ Skipped: {summary['SKIPPED']}"
    )
    if summary["ERROR"] > 0:
        print(f"âš ï¸ {summary['ERROR']} evaluations had errors - check detailed report")

    print("\nðŸ“Š Token Usage Summary:")
    print(
        f"Judge LLM: {summary['total_judge_llm_tokens']:,} tokens "
        f"(Input: {summary['total_judge_llm_input_tokens']:,}, "
        f"Output: {summary['total_judge_llm_output_tokens']:,})"
    )
    if api_tokens:
        print(
            f"API Calls: {api_tokens['total_api_tokens']:,} tokens "
            f"(Input: {api_tokens['total_api_input_tokens']:,}, "
            f"Output: {api_tokens['total_api_output_tokens']:,})"
        )
        total = summary["total_judge_llm_tokens"] + api_tokens["total_api_tokens"]
        print(f"Total: {total:,} tokens")


def run_evaluation(  # pylint: disable=too-many-locals
    eval_args: argparse.Namespace,
) -> Optional[dict[str, int]]:
    """Run the complete evaluation pipeline.

    Args:
        eval_args: Parsed command line arguments

    Returns:
        dict: Summary statistics with keys TOTAL, PASS, FAIL, ERROR, SKIPPED
    """
    print("ðŸš€ LightSpeed Evaluation Framework")
    print("=" * 50)

    try:
        print("ðŸ”§ Loading Configuration & Setting up environment...")
        loader = ConfigLoader()
        system_config = loader.load_system_config(eval_args.system_config)

        # Clear caches if cache warmup mode is enabled
        if eval_args.cache_warmup:
            print("\nðŸ”¥ Cache warmup mode: Clearing existing caches...")
            _clear_caches(system_config)

        # Import heavy modules after environment is configured
        print("\nðŸ“‹ Loading Heavy Modules...")
        # pylint: disable=import-outside-toplevel
        from lightspeed_evaluation.core.output import OutputHandler
        from lightspeed_evaluation.core.output.statistics import (
            calculate_api_token_usage,
            calculate_basic_stats,
        )
        from lightspeed_evaluation.core.system import DataValidator
        from lightspeed_evaluation.pipeline.evaluation import EvaluationPipeline

        # pylint: enable=import-outside-toplevel
        print("âœ… Configuration loaded & Setup is done !")

        # Load, filter, and validate evaluation data
        evaluation_data = DataValidator(
            api_enabled=system_config.api.enabled,
            fail_on_invalid_data=system_config.core.fail_on_invalid_data,
        ).load_evaluation_data(
            eval_args.eval_data,
            tags=eval_args.tags,
            conv_ids=eval_args.conv_ids,
        )

        print(
            f"âœ… System config: {system_config.llm.provider}/{system_config.llm.model}"
        )

        # Handle case where no conversations match the filter
        if len(evaluation_data) == 0:
            print("\nâš ï¸ No conversation groups matched the filter criteria")
            print("   Nothing to evaluate - returning empty results")
            return {"TOTAL": 0, "PASS": 0, "FAIL": 0, "ERROR": 0, "SKIPPED": 0}

        # Run evaluation pipeline
        print("\nâš™ï¸ Initializing Evaluation Pipeline...")
        pipeline = EvaluationPipeline(loader, eval_args.output_dir)

        print("\nðŸ”„ Running Evaluation...")
        try:
            results = pipeline.run_evaluation(evaluation_data, eval_args.eval_data)
        finally:
            pipeline.close()

        # Generate reports
        print("\nðŸ“Š Generating Reports...")
        output_handler = OutputHandler(
            output_dir=eval_args.output_dir or system_config.output.output_dir,
            base_filename=system_config.output.base_filename,
            system_config=system_config,
        )
        output_handler.generate_reports(results, evaluation_data)

        print("\nðŸŽ‰ Evaluation Complete!")
        print(f"ðŸ“Š {len(results)} evaluations completed")
        print(f"ðŸ“ Reports generated in: {output_handler.output_dir}")

        # Final Summary
        summary = calculate_basic_stats(results)
        api_tokens = (
            calculate_api_token_usage(evaluation_data)
            if system_config.api.enabled
            else None
        )
        _print_summary(summary, api_tokens)

        return {
            "TOTAL": summary["TOTAL"],
            "PASS": summary["PASS"],
            "FAIL": summary["FAIL"],
            "ERROR": summary["ERROR"],
            "SKIPPED": summary["SKIPPED"],
        }

    except (FileNotFoundError, ValueError, RuntimeError, DataValidationError) as e:
        print(f"\nâŒ Evaluation failed: {e}")
        traceback.print_exc()
        return None


def main() -> int:
    """Command line interface."""
    parser = argparse.ArgumentParser(
        description="LightSpeed Evaluation Framework / Tool",
    )
    parser.add_argument(
        "--system-config",
        default="config/system.yaml",
        help="Path to system configuration file (default: config/system.yaml)",
    )
    parser.add_argument(
        "--eval-data",
        default="config/evaluation_data.yaml",
        help="Path to evaluation data file (default: config/evaluation_data.yaml)",
    )
    parser.add_argument("--output-dir", help="Override output directory (optional)")
    parser.add_argument(
        "--tags",
        nargs="+",
        default=None,
        help="Filter by tags (run conversation groups with matching tags)",
    )
    parser.add_argument(
        "--conv-ids",
        nargs="+",
        default=None,
        help="Filter by conversation group IDs (run only specified conversations)",
    )
    parser.add_argument(
        "--cache-warmup",
        action="store_true",
        help="Enable cache warmup mode - rebuild caches without reading existing entries",
    )

    eval_args = parser.parse_args()

    summary = run_evaluation(eval_args)
    return 0 if summary is not None else 1


if __name__ == "__main__":
    sys.exit(main())
