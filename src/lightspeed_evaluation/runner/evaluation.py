"""LightSpeed Evaluation Framework - Main Evaluation Runner."""

import argparse
import sys
import traceback
from pathlib import Path
from typing import Dict, Optional

from ..core.config import ConfigLoader, DataValidator, setup_environment_variables
from ..core.output import OutputHandler
from ..core.output.statistics import calculate_basic_stats
from ..drivers.evaluation import EvaluationDriver


def run_evaluation(
    system_config_path: str, evaluation_data_path: str, output_dir: Optional[str] = None
) -> Optional[Dict[str, int]]:
    """
    Run the complete evaluation pipeline using EvaluationDriver.

    Args:
        system_config_path: Path to system.yaml
        evaluation_data_path: Path to evaluation_data.yaml
        output_dir: Optional override for output directory

    Returns:
        dict: Summary statistics with keys TOTAL, PASS, FAIL, ERROR
    """
    print("üöÄ LightSpeed Evaluation Framework")
    print("=" * 50)

    try:
        # Step 1: Load configuration
        print("\nüìã Loading Configuration...")
        loader = ConfigLoader()
        system_config = loader.load_system_config(system_config_path)

        data_validator = DataValidator()
        evaluation_data = data_validator.load_evaluation_data(evaluation_data_path)

        print(
            f"‚úÖ System config: {system_config.llm_provider}/{system_config.llm_model}"
        )
        print(f"‚úÖ Evaluation data: {len(evaluation_data)} conversation groups")

        # Step 2: Initialize evaluation driver (core controller)
        print("\n‚öôÔ∏è Initializing Evaluation Driver...")
        driver = EvaluationDriver(loader)

        # Step 3: Run evaluation (driver controls the flow)
        print("\nüîÑ Running Evaluation...")
        results = driver.run_evaluation(evaluation_data)

        # Step 4: Generate reports
        print("\nüìä Generating Reports...")
        output_handler = OutputHandler(
            output_dir=output_dir or system_config.output_dir,
            base_filename=system_config.base_filename,
            system_config=system_config,
        )

        output_handler.generate_reports(
            results, include_graphs=system_config.include_graphs
        )

        print("\nüéâ Evaluation Complete!")
        print(f"üìä {len(results)} evaluations completed")
        print(f"üìÅ Reports generated in: {output_handler.output_dir}")

        # Calculate and show summary
        summary = calculate_basic_stats(results)

        print(
            f"‚úÖ Pass: {summary['PASS']}, ‚ùå Fail: {summary['FAIL']}, ‚ö†Ô∏è Error: {summary['ERROR']}"
        )

        if summary["ERROR"] > 0:
            print(
                f"‚ö†Ô∏è {summary['ERROR']} evaluations had errors - check detailed report"
            )

        return {
            "TOTAL": summary["TOTAL"],
            "PASS": summary["PASS"],
            "FAIL": summary["FAIL"],
            "ERROR": summary["ERROR"],
        }

    except (FileNotFoundError, ValueError, RuntimeError) as e:
        print(f"\n‚ùå Evaluation failed: {e}")
        traceback.print_exc()
        return None


def main() -> int:
    """Command line interface."""
    parser = argparse.ArgumentParser(
        description="LightSpeed Evaluation Framework / Tool",
    )
    parser.add_argument(
        "--system-config",
        default="config/system.yaml",
        help="Path to system configuration file (default: config/system.yaml)",
    )
    parser.add_argument(
        "--eval-data",
        default="config/evaluation_data.yaml",
        help="Path to evaluation data file (default: config/evaluation_data.yaml)",
    )
    parser.add_argument("--output-dir", help="Override output directory (optional)")

    args = parser.parse_args()

    # CRITICAL: Setup environment variables from system config FIRST
    setup_environment_variables(args.system_config)

    # Validate input files exist
    if not Path(args.system_config).exists():
        print(f"‚ùå System config file not found: {args.system_config}")
        return 1

    if not Path(args.eval_data).exists():
        print(f"‚ùå Evaluation data file not found: {args.eval_data}")
        return 1

    # Run evaluation
    summary = run_evaluation(args.system_config, args.eval_data, args.output_dir)

    return 0 if summary is not None else 1


if __name__ == "__main__":
    sys.exit(main())
